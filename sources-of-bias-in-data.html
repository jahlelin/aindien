<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Sources of Bias in Data - Jason&#x27;s Computing Guides</title><meta name="description" content="This is a guide of sources of bias in data."><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://aindien.com/sources-of-bias-in-data.html"><link rel="alternate" type="application/atom+xml" href="https://aindien.com/feed.xml" title="Jason&#x27;s Computing Guides - RSS"><link rel="alternate" type="application/json" href="https://aindien.com/feed.json" title="Jason&#x27;s Computing Guides - JSON"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--logo-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--menu-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol"}</style><link rel="stylesheet" href="https://aindien.com/assets/css/style.css?v=166a31b4480c68773db8a06507216db7"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://aindien.com/sources-of-bias-in-data.html"},"headline":"Sources of Bias in Data","datePublished":"2025-07-10T20:34-05:00","dateModified":"2025-07-10T20:36-05:00","description":"This is a guide of sources of bias in data.","author":{"@type":"Person","name":"Jason Moore","url":"https://aindien.com/authors/jason-moore/"},"publisher":{"@type":"Organization","name":"Jason Moore"}}</script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-393JFJ482L"></script><script>window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-393JFJ482L');</script><script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/5aefa50c3a5900492b165a83f/93dcd5a76da18d3becc7e677f.js");</script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://aindien.com/">Jason&#x27;s Computing Guides</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="https://aindien.com/about-me.html" target="_self">Andromeda</a></li><li><a href="https://aindien.com/c/" target="_self">C++</a></li><li><a href="https://aindien.com/linux/" target="_self">Linux</a></li><li><a href="https://aindien.com/networking/" target="_self">Networking</a></li><li><a href="https://aindien.com/git/" target="_self">Git</a></li><li><a href="https://aindien.com/python/" target="_self">Python</a></li><li><a href="https://aindien.com/ai/" target="_self">AI</a></li></ul></nav><div class="search"><div class="search__overlay js-search-overlay"><div class="search__overlay-inner"><form action="https://aindien.com/search.html" class="search__form"><input class="search__input js-search-input" type="search" name="q" placeholder="search..." aria-label="search..." autofocus="autofocus"></form><button class="search__close js-search-close" aria-label="Close">Close</button></div></div><button class="search__btn js-search-btn" aria-label="Search"><svg role="presentation" focusable="false"><use xlink:href="https://aindien.com/assets/svg/svg-map.svg#search"/></svg></button></div></header><main><article class="post"><div class="hero"><figure class="hero__image hero__image--overlay"><img src="https://aindien.com/media/website/computing-logo-2.jpg" srcset="https://aindien.com/media/website/responsive/computing-logo-2-xs.jpg 300w, https://aindien.com/media/website/responsive/computing-logo-2-sm.jpg 480w, https://aindien.com/media/website/responsive/computing-logo-2-md.jpg 768w, https://aindien.com/media/website/responsive/computing-logo-2-lg.jpg 1024w, https://aindien.com/media/website/responsive/computing-logo-2-xl.jpg 1360w, https://aindien.com/media/website/responsive/computing-logo-2-2xl.jpg 1600w" sizes="(max-width: 1600px) 100vw, 1600px" loading="eager" alt=""></figure><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2025-07-10T20:34">July 10, 2025</time></div><h1>Sources of Bias in Data</h1><div class="post__meta post__meta--author"><a href="https://aindien.com/authors/jason-moore/" class="feed__author invert">Jason Moore</a></div></div></header></div><div class="wrapper post__entry"><p>This is a guide of sources of bias in data.</p><p><a target="_blank" href="https://amzn.to/44LauL7" rel="noopener">This is the Ankr Power Bank I have. It has been great and reliable when I go on trips or when I get on my laptop to write somewhere away from home.</a></p><p>Now that we have looked at the types of data bias in machine learning, we can talk through the sources. Although there is not a way to make an environment completely free of bias, it is important to be able to identify and reduce the amount of bias found in any model or dataset.</p><p> </p><p>Sources of bias can come from the humans who are responsible for creating the model or generating the data, from the data not being robust and lacking the proper amount of data points to represent a situation accurately, or from the way the model builds upon what users input into the model.</p><p> </p><p>Some of the common sources of data bias are: human subconscious prejudices and assumptions; lack of data points creating outputs that misrepresent the situation; and bias feedback loops from the ways users interact with a model that perpetuate bias.</p><p> </p><p>We will look at each of these in more detail as well as their impact. Data bias can occur on various stages of the AI process. Data collection: biases may come from distorted survey questions, incomplete data collection, or favoring certain data sources, which can lead to incomplete or distorted datasets that influence AI models to make inaccurate predictions. Historical biases originate from existing prejudices and inequalities within historical records or datasets.</p><p> </p><p>Sampling methods: bias in sampling methods occur when samples are selected in a way that does not accurately represent the broader population, which can lead to models that struggle to generalize to real-world scenarios, particularly for underrepresented groups.</p><p> </p><p>Bias can occur during data aggregation when data is combined without accounting for subgroup variations, which may lead to obscure disparities among subgroups causing models to overlook specific patterns or needs within the data. Bias can occur during data labeling from subjective or culturally influenced labeling.</p><p> </p><p>This can result in inaccurate predictions or classifications when labels reflect subjective judgements. Data preprocessing bias can come from decisions like handling missing values or outliers, and such biased choices can introduce artifacts into the data affecting the performance and fairness of models. When datasets that contain bias are applied to AI and machine learning models, the biases can have a large impact on the ability to make ethical decisions within the data.</p><p> </p><p>One issue from these biases existing is that the models trained on data chosen or gathered by humans, and models trained from historical data about human activities, can include insensitive connections or biases. Another issue is that user-generated data can lead to biased feedback loops. Machine learning algorithms could conclude culturally offensive or insensitive information. Models that are trained with data that has been created or gathered by humans, can inherit different cultural and social biases.</p><p> </p><p>For example, data from historical sources or past news articles, may produce outputs that contain racial or social language biases. These outputs, in turn, have negative impacts on the decisions being made. For example, an algorithm used to support hiring might be trained to seek applicants that use language commonly associated with men. Data generated by users can produce feedback loops that are rooted in cultural biases. For example, the more users search keywords together, the more the results contain those words, whether the words are searched together or not.</p><p> </p><p>When machine learning algorithms make statistical connections, they might produce outcomes that are unlawful or inappropriate. For example, a model looking at loans across a range of ages, could identify that an age group is more likely to default. That information could not be used to make decisions without breaching discrimination laws. When using AI and machine learning models, there are ethical concerns to consider.</p><p> </p><p>There are unfortunately many cases of bias in AI, and they have large negative impacts for people. This situation occurs in numerous industries, with people being discriminated against in varying ways.</p><p> </p><p>Our first example of AI bias is Amazon's hiring algorithm case study. Because of its success with automation elsewhere at Amazon, there is a push to automate parts of the hiring process. Sorting and scoring resumes, and then recommending the highest-scoring candidates to hiring managers and other HR stakeholders was the objective for the program.</p><p> </p><p>Using AI and machine learning, resumes could be analyzed for the right terms and then given preference over resumes that did not include these terms, or at least rank higher than those resumes that included more basic terms and terms that are associated with lower-level skills. The models were trained on data that was heavily focused on male-related terms and their resumes.</p><p> </p><p>So, in turn, that is what the model preferred to other options. The model also built on the data and began to devalue resumes that included the word women or lacked male references. The algorithm was scoring highly qualified women lower than their equally qualified male counterparts. This was leading to hiring managers being given information about ranking that was discriminatory against women.</p><p> </p><p>The program was ultimately abandoned when it could not be corrected. An investigation by The Markup in 2019 found that applicants of color were 40-80% more likely to be denied loan approvals compared to white applicants. Even in cases where the applicants were identical, the white applicants were approved and the applicants that were Latino, Black, Asian, and others were denied.</p><p> </p><p>Outside of the creators of the algorithms used to underwrite the loans, there were few who knew how the algorithm works. This has led to criticism from the public and an impact to the trust in the model that is producing these results. In 2022, there was a viral message that highlighted Twitter's discrimination when selecting which part of a photo to show.</p><p> </p><p>The feature was auto cropping pictures to focus on white people over people of color. When the issue was made public, twitter released an explanation to show how it happened and to take accountability for the issue. By being transparent, Twitter was able to maintain trust in their product and alter the program to include a wider variety of source data, so the issue would not continue.</p></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on July 10, 2025</p><ul class="post__tag"><li><a href="https://aindien.com/ai/">AI</a></li></ul><div class="post__share"><a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Faindien.com%2Fsources-of-bias-in-data.html" class="js-share facebook" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://aindien.com/assets/svg/svg-map.svg#facebook"/></svg> <span>Facebook</span> </a><a href="https://twitter.com/share?url=https%3A%2F%2Faindien.com%2Fsources-of-bias-in-data.html&amp;via=_Aindien&amp;text=Sources%20of%20Bias%20in%20Data" class="js-share twitter" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://aindien.com/assets/svg/svg-map.svg#twitter"/></svg> <span>Twitter</span></a></div><div class="post__bio bio"><div class="bio__info"><h3 class="bio__name"><a href="https://aindien.com/authors/jason-moore/" class="invert" rel="author">Jason Moore</a></h3></div></div></footer></article><nav class="post__nav"><div class="post__nav-inner"><div class="post__nav-prev"><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://aindien.com/assets/svg/svg-map.svg#arrow-prev"/></svg> <a href="https://aindien.com/data-bias-in-artificial-intelligence.html" class="invert post__nav-link" rel="prev"><span>Previous</span> Data Bias in Artificial Intelligence</a></div><div class="post__nav-next"><a href="https://aindien.com/transparency-and-fairness-in-ai-model-development.html" class="invert post__nav-link" rel="next"><span>Next</span> Transparency and Fairness in AI Model Development </a><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://aindien.com/assets/svg/svg-map.svg#arrow-next"/></svg></div></div></nav><div class="post__related related"><div class="wrapper"><h2 class="h5 related__title">You should also read:</h2><article class="related__item"><div class="feed__meta"><time datetime="2025-07-10T07:25" class="feed__date">July 10, 2025</time></div><h3 class="h1"><a href="https://aindien.com/data-bias-in-artificial-intelligence.html" class="invert">Data Bias in Artificial Intelligence</a></h3></article><article class="related__item"><div class="feed__meta"><time datetime="2025-07-08T19:20" class="feed__date">July 8, 2025</time></div><h3 class="h1"><a href="https://aindien.com/how-ai-works-with-data.html" class="invert">How AI Works with Data</a></h3></article></div></div><div class="banner banner--after-post"><div class="wrapper"><link href="//cdn-images.mailchimp.com/embedcode/classic-10_7.css" rel="stylesheet" type="text/css"><style type="text/css">#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }
	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */</style><div id="mc_embed_signup"><form action="https://aindien.us11.list-manage.com/subscribe/post?u=5aefa50c3a5900492b165a83f&amp;id=1c348b805a" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate><div id="mc_embed_signup_scroll"><h2>Subscribe</h2><div class="indicates-required"><span class="asterisk">*</span> indicates required</div><div class="mc-field-group"><label for="mce-EMAIL">Email Address <span class="asterisk">*</span></label> <input type="email" name="EMAIL" class="required email" id="mce-EMAIL"></div><div class="mc-field-group"><label for="mce-FNAME">First Name</label> <input type="text" name="FNAME" id="mce-FNAME"></div><div class="mc-field-group"><label for="mce-LNAME">Last Name</label> <input type="text" name="LNAME" id="mce-LNAME"></div><div id="mce-responses" class="clear"><div class="response" id="mce-error-response" style="display:none"></div><div class="response" id="mce-success-response" style="display:none"></div></div><div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_5aefa50c3a5900492b165a83f_1c348b805a" tabindex="-1"></div><div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div></div></form></div><script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script></div></div></main><footer class="footer"><div class="footer__copyright"><p>Jason Moore 2022</p></div><button class="footer__bttop js-footer__bttop" aria-label="Back to top"><svg><title>Back to top</title><use xlink:href="https://aindien.com/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.top',
   };</script><script defer="defer" src="https://aindien.com/assets/js/scripts.min.js?v=f4c4d35432d0e17d212f2fae4e0f8247"></script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>